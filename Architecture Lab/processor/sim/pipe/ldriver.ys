#######################################################################
# Test for copying block of size 63;
#######################################################################
	.pos 0
main:	irmovq Stack, %rsp  	# Set up stack pointer

	# Set up arguments for copy function and then invoke it
	irmovq $63, %rdx		# src and dst have 63 elements
	irmovq dest, %rsi	# dst array
	irmovq src, %rdi	# src array
    # corrupt all the unused registers to prevent assumptions
    irmovq $0x5710331, %rax
    irmovq $0x5710331, %rbx
    irmovq $0x5710331, %rcx
    irmovq $0x5710331, %rbp
    irmovq $0x5710331, %r8
    irmovq $0x5710331, %r9
    irmovq $0x5710331, %r10
    irmovq $0x5710331, %r11
    irmovq $0x5710331, %r12
    irmovq $0x5710331, %r13
    irmovq $0x5710331, %r14
	call to_binary_string		
	 
	halt			# should halt with sum in %rax
StartFun:
# Author: Mehmet Rüçhan Yavuzdemir, 2522159

# First, I implemented loop unrolling and precomputation, which reduced the CPE from 386 to 59. The reasong behind it is that calculating the right shift every 
# time in a separate loop slows down the benchmark time a lot, and very inefficient. Instead, I precomputed multiples of 2, and put them into registers. Then, 
# according to the andq result, I put '0' or '1'. This way, I saw a pretty good speed up. I avoided computations in the hot regions, tried to distribute them.

# Next I optimized a small but effective thing. I realized that assigning $48 and $49 at each step was unnecessary as I do it more frequent than andq checkings. 
# I reserved %r13 and %r14 for $48 and $49. This optimization further reduced the CPE from 59 to 51, 8 CPE speedup. Finally, I made the application 7.5x faster! 

##################################################################
# %rdi = arr, %rsi = buff, %rdx = len
to_binary_string:
    # sum = 0
    xorq %rax,%rax

    # len <= 0 ?
    andq %rdx,%rdx
    jle Done

While_Loop:    
    # val = *arr
    mrmovq (%rdi), %rbx

    # sum += val
    addq %rbx, %rax 

    irmovq $128, %rbp
    irmovq $64, %r8
    irmovq $32, %r9
    irmovq $16, %r10
    irmovq $8, %r11
    irmovq $4, %r12

    # irmovq $2, %r13
    # irmovq $1, %r14

    irmovq $48, %r13
    irmovq $49, %r14

E1:
    # = '0'
    rmmovq %r13, (%rsi)

    andq %rbx, %rbp
    je E2 # & == 0 ?

    # = '1'
    rmmovq %r14, (%rsi)

E2:
    # = '0'
    rmmovq %r13, 1(%rsi)

    andq %rbx, %r8
    je E3 # & == 0 ?

    # = '1'
    rmmovq %r14, 1(%rsi)

E3:
    # = '0'    
    rmmovq %r13, 2(%rsi)

    andq %rbx, %r9
    je E4 # & == 0 ?

    # = '1'
    rmmovq %r14, 2(%rsi)

E4:
    # = '0'
    rmmovq %r13, 3(%rsi)

    andq %rbx, %r10
    je E5 # & == 0 ?

    # = '1'
    rmmovq %r14, 3(%rsi)

E5:
    # = '0'
    rmmovq %r13, 4(%rsi)

    andq %rbx, %r11
    je E6 # & == 0 ?

    # = '1'
    rmmovq %r14, 4(%rsi)

E6:
    # = '0'
    rmmovq %r13, 5(%rsi)

    andq %rbx, %r12
    je E7 # & == 0 ?

    # = '1'
    rmmovq %r14, 5(%rsi)

E7:
    # = '0'
    rmmovq %r13, 6(%rsi)

    irmovq $2, %rcx
    andq %rbx, %rcx
    je E8 # & == 0 ?

    # = '1'
    rmmovq %r14, 6(%rsi)

E8:
    # = '0'
    rmmovq %r13, 7(%rsi)

    irmovq $1, %rcx
    andq %rbx, %rcx
    je Updates # & == 0 ?

    # = '1'
    rmmovq %r14, 7(%rsi)

Updates:
    irmovq $8, %rcx

    # arr++
    addq %rcx, %rdi 

    # buff++
    addq %rcx, %rsi 
 
    # --len > 0 ?
    irmovq $1, %rcx
    subq %rcx, %rdx
    jg While_Loop
    
Done:
    ret
End:
EndFun:

###############################
# Source and destination blocks 
###############################
	.align 8
src:
	.quad 1
	.quad 2
	.quad 3
	.quad 4
	.quad 5
	.quad 6
	.quad 7
	.quad 8
	.quad 9
	.quad 10
	.quad 11
	.quad 12
	.quad 13
	.quad 14
	.quad 15
	.quad 16
	.quad 17
	.quad 18
	.quad 19
	.quad 20
	.quad 21
	.quad 22
	.quad 23
	.quad 24
	.quad 25
	.quad 26
	.quad 27
	.quad 28
	.quad 29
	.quad 30
	.quad 31
	.quad 32
	.quad 33
	.quad 34
	.quad 35
	.quad 36
	.quad 37
	.quad 38
	.quad 39
	.quad 40
	.quad 41
	.quad 42
	.quad 43
	.quad 44
	.quad 45
	.quad 46
	.quad 47
	.quad 48
	.quad 49
	.quad 50
	.quad 51
	.quad 52
	.quad 53
	.quad 54
	.quad 55
	.quad 56
	.quad 57
	.quad 58
	.quad 59
	.quad 60
	.quad 61
	.quad 62
	.quad 63
	.quad 0xbcdefa # This shouldn't get moved

	.align 16
Predest:
	.quad 0xbcdefa
dest:
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
Postdest:
	.quad 0xdefabc

.align 8
# Run time stack
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0

Stack:
